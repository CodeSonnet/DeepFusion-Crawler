import time
import json
import random
import os
import pandas as pd  # å¼•å…¥å¼ºå¤§çš„æ•°æ®å¤„ç†åº“
from selenium import webdriver
from selenium.webdriver.common.by import By

# --- é…ç½®åŒºåŸŸ ---
# æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨ï¼Œé˜²æ­¢è¢«äº¬ä¸œè¯†åˆ«ä¸ºæœºå™¨äºº
options = webdriver.ChromeOptions()
options.add_argument('--disable-blink-features=AutomationControlled')
# éšè— "Chromeæ­£åœ¨å—åˆ°è‡ªåŠ¨è½¯ä»¶çš„æ§åˆ¶" æç¤º
options.add_experimental_option("excludeSwitches", ["enable-automation"])
options.add_experimental_option('useAutomationExtension', False)

def start_crawler():
    # 1. å‡†å¤‡å·¥ä½œï¼šç¡®ä¿ data æ–‡ä»¶å¤¹å­˜åœ¨
    if not os.path.exists('data'):
        os.makedirs('data')
        print("âœ… å·²è‡ªåŠ¨åˆ›å»º data æ–‡ä»¶å¤¹")

    # 2. è¯»å–æˆ‘ä»¬è¦çˆ¬çš„å•†å“åˆ—è¡¨
    try:
        # å¦‚æœ seed_products.json åœ¨ä¸Šä¸€çº§ç›®å½•ï¼Œè¿™é‡Œè¦åšå…¼å®¹
        config_path = 'seed_products.json' 
        if not os.path.exists(config_path) and os.path.exists('../seed_products.json'):
            config_path = '../seed_products.json'
            
        with open(config_path, 'r', encoding='utf-8') as f:
            products = json.load(f)
    except FileNotFoundError:
        print("âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ° seed_products.json æ–‡ä»¶ï¼è¯·ç¡®è®¤å®ƒåœ¨æ­£ç¡®çš„ä½ç½®ã€‚")
        return

    # 3. å¯åŠ¨æµè§ˆå™¨
    print("ğŸš€ å¯åŠ¨æµè§ˆå™¨ä¸­...")
    driver = webdriver.Chrome(options=options)
    
    # ç”¨äºä¸´æ—¶å­˜å‚¨æ‰€æœ‰çˆ¬å–åˆ°çš„æ•°æ®
    all_comments = []

    for product in products:
        print(f"\n------ æ­£åœ¨çˆ¬å–: {product['product_name']} ------")
        driver.get(product['jd_url'])
        
        # --- å…³é”®ï¼šäººå·¥å¹²é¢„æ—¶é—´ ---
        # äº¬ä¸œæœ‰æ—¶ä¼šå¼¹å‡ºç™»å½•çª—å£ï¼Œè¿™é‡Œç•™ç»™ä½ 15ç§’æ‰‹åŠ¨æ‰«ç æˆ–å…³æ‰å¼¹çª—
        print("â³ ç­‰å¾…é¡µé¢åŠ è½½... (å¦‚æœä½ çœ‹åˆ°ç™»å½•å¼¹çª—ï¼Œè¯·æ‰‹åŠ¨å…³æ‰æˆ–å¿«é€Ÿæ‰«ç ï¼Œä½ æœ‰15ç§’æ—¶é—´)")
        time.sleep(15) 

        # 4. æ¨¡æ‹Ÿç‚¹å‡»â€œå•†å“è¯„ä»·â€æ ‡ç­¾
        try:
            # å°è¯•ç‚¹å‡»â€œå•†å“è¯„ä»·â€æŒ‰é’®ï¼Œå®šä½æ›´ç²¾å‡†
            comment_tab = driver.find_element(By.XPATH, "//li[@data-anchor='#comment']")
            comment_tab.click()
            print("âœ… å·²ç‚¹å‡»â€˜å•†å“è¯„ä»·â€™æ ‡ç­¾")
            time.sleep(2)
        except:
            print("âš ï¸ æœªç‚¹å‡»è¯„ä»·æ ‡ç­¾ï¼Œå¯èƒ½å·²è‡ªåŠ¨è·³è½¬æˆ–é¡µé¢ç»“æ„æ”¹å˜ï¼Œå°è¯•ç›´æ¥æ»šåŠ¨")

        # 5. æ¨¡æ‹Ÿäººæ‰‹æ»šåŠ¨é¡µé¢ (æ…¢æ…¢æ»šï¼Œè®©æ•°æ®åŠ è½½å‡ºæ¥)
        for i in range(3):
            driver.execute_script(f"window.scrollTo(0, document.body.scrollHeight * {i/3 + 0.3});")
            time.sleep(random.uniform(1, 2))

        # 6. æŠ“å–è¯„è®ºåˆ—è¡¨
        # äº¬ä¸œè¯„è®ºåŒºçš„ class åå­—é€šå¸¸æ˜¯ comment-item
        comments = driver.find_elements(By.CLASS_NAME, 'comment-item')
        print(f"ğŸ‘€ æœ¬é¡µå‘ç° {len(comments)} æ¡è¯„è®ºï¼Œå¼€å§‹è§£æ...")
        
        for item in comments:
            try:
                # æå–å†…å®¹
                content_ele = item.find_element(By.CLASS_NAME, 'comment-con')
                content = content_ele.text.replace('\n', ' ') # å»æ‰æ¢è¡Œç¬¦
                
                # å°è¯•æå–æ—¶é—´ (äº¬ä¸œçš„æ—¶é—´é€šå¸¸åœ¨ order-info æˆ– comment-time ç±»ä¼¼ç»“æ„é‡Œï¼Œè¿™é‡Œåšä¸ªé€šç”¨å°è¯•)
                # è¿™é‡Œçš„ class å¯èƒ½ä¼šå˜ï¼Œå¦‚æœæŠ“ä¸åˆ°ä¹Ÿæ²¡å…³ç³»ï¼Œå…ˆä¿è¯ä»£ç ä¸å´©
                try:
                    date_str = item.find_element(By.CLASS_NAME, 'order-info').text
                except:
                    date_str = "æœªçŸ¥æ—¶é—´"

                # å¯¹åº”ã€Šæ•°æ®åº“ç»“æ„.pdfã€‹é‡Œçš„å­—æ®µ
                data = {
                    "product_id": product.get('sku_id', 'unknown'), # ä»jsoné‡Œæ‹¿SKU
                    "product_name": product['product_name'],
                    "platform": "jd",
                    "content": content,
                    "raw_info": date_str, # æš‚æ—¶æŠŠæ—¶é—´å’Œå…¶ä»–ä¿¡æ¯å­˜åœ¨è¿™é‡Œ
                    "crawl_time": time.strftime("%Y-%m-%d %H:%M:%S")
                }
                all_comments.append(data)
                print(f"   [æˆåŠŸ] {content[:15]}...")
            except Exception as e:
                # æŸä¸€æ¡å‡ºé”™äº†è·³è¿‡ï¼Œä¸è¦å¡æ­»
                continue
        
        time.sleep(random.uniform(2, 4))

    driver.quit()
    print("\nâœ… çˆ¬å–ç»“æŸï¼Œæµè§ˆå™¨å·²å…³é—­ã€‚")

    # 7. ä¿å­˜æ•°æ®åˆ° CSV (è¿™æ‰æ˜¯é‡ç‚¹ï¼)
    if all_comments:
        df = pd.DataFrame(all_comments)
        # encoding='utf-8_sig' æ˜¯ä¸ºäº†é˜²æ­¢ Excel æ‰“å¼€ä¸­æ–‡ä¹±ç 
        save_path = 'data/jd_comments.csv'
        df.to_csv(save_path, index=False, encoding='utf-8_sig')
        print(f"ğŸ‰ æˆåŠŸï¼æ•°æ®å·²ä¿å­˜åˆ°: {os.path.abspath(save_path)}")
        print("ğŸ’¡ ä½ ç°åœ¨å¯ä»¥å» data æ–‡ä»¶å¤¹é‡ŒåŒå‡»æ‰“å¼€è¿™ä¸ª CSV æ–‡ä»¶æŸ¥çœ‹æˆæœäº†ï¼")
    else:
        print("âš ï¸ æœ¬æ¬¡æ²¡æœ‰æŠ“å–åˆ°ä»»ä½•æ•°æ®ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–é¡µé¢æ˜¯å¦å¼¹å‡ºäº†éªŒè¯ç ã€‚")

if __name__ == "__main__":
    start_crawler()